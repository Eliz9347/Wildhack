# -*- coding: utf-8 -*-
"""task_2_plus_team.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vXTNlm8bbgXFDDKvkp4PKr00eD4a4fcu
"""

#Ссылка на гугл колаб  https://colab.research.google.com/drive/1vXTNlm8bbgXFDDKvkp4PKr00eD4a4fcu?usp=sharing

# Commented out IPython magic to ensure Python compatibility.
# %pip install tqdm

# Commented out IPython magic to ensure Python compatibility.
# %pip install pyspellchecker

# Commented out IPython magic to ensure Python compatibility.
# %pip install spacy

# Commented out IPython magic to ensure Python compatibility.
# %pip install nltk

pip install pymorphy2

import numpy as np
import pandas as pd

import tqdm

from tqdm.notebook import tqdm

import re

import pymorphy2

from spellchecker import SpellChecker

import spacy

from nltk.stem import PorterStemmer

tqdm.pandas()





#КОНСТАНТЫ
nBestUsers = 50

BIG_QUESTION = 100

NORMING_CONST = 500

PROXIMITY = 0.84





# РУССКОЯЗЫЧНЫЙ класс проверяющий опечтаки
spell_russian = SpellChecker(language='ru')

# АНГЛОЯЗЫЧНЫЙ класс проверяющий опечатки в английском
spell_english = SpellChecker()

#Инициализация лемматизаторов
russian_lemmatizer = pymorphy2.MorphAnalyzer()

english_lemmatizer = PorterStemmer()



#Функции приведения к определенной раскладке
def to_russian (text):
    layout = dict(zip(map(ord, '''qwertyuiop[]asdfghjkl;'zxcvbnm,./`QWERTYUIOP{}ASDFGHJKL:"ZXCVBNM<>?~'''),
                               '''йцукенгшщзхъфывапролджэячсмитьбю.ёЙЦУКЕНГШЩЗХЪФЫВАПРОЛДЖЭЯЧСМИТЬБЮ,Ё'''))
    return text.translate(layout)

def to_english (text):
    layout = dict(zip(map(ord, '''йцукенгшщзхъфывапролджэячсмитьбю.ёЙЦУКЕНГШЩЗХЪФЫВАПРОЛДЖЭЯЧСМИТЬБЮ,Ё'''),
                               '''qwertyuiop[]asdfghjkl;'zxcvbnm,./`QWERTYUIOP{}ASDFGHJKL:"ZXCVBNM<>?~'''))
    return text.translate(layout)

#Демонстрация работы приведения строки к русской раскладке
text = 'bbb ввв'
print(to_russian(text))

#Принимает на вход строку
def correct_the_word (this_str,  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer):
  russian_version = to_russian (this_str)
  english_version = to_english (this_str)

  #для русской версии
  #Если множество неизвестных пустое, значит слово известно (УСПЕХ!)
  #!!! - закомментировано - т.к. занимает много времени word_ru = spell_russian.correction(russian_version)
  word_ru = russian_version
  if not spell_russian.unknown (  [    word_ru    ]   ):
    normal_word_russian = russian_lemmatizer.parse(word_ru)[0].normal_form
    return normal_word_russian
  


  #для английской
  #!!! - закомментировано - т.к. занимает много времени word_ru word_en = spell_english.correction(english_version)
  word_en = english_version
  if not spell_english.unknown (  [   word_en    ]   ):
    normal_word_english = english_lemmatizer.stem(word_en)
    return normal_word_english
  
  #Если совпадений не найдено, возвращаем исходное (или это очень суровая опечатка), или так и надо
  return this_str

def process_string (this_string,  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer):
  list_of_words = this_string.split()
  #print (list_of_words)

  for i in range (0, len (list_of_words)):
    list_of_words[i]  = correct_the_word (list_of_words[i],  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer)

    #if ((i % 100) == 0):
    #  print (i)

  #print (list_of_words)
  list_of_words.sort()
  #print (list_of_words)
  #print (len (list_of_words))

  processed_string = ""
  for word in list_of_words:
    #print (word)
    processed_string += word + " "
  
  processed_string = processed_string.strip()
  #print (processed_string)
  return processed_string

# Для датасета эти функции запускаются отдельно и датасет хранится уже лемматизированный и с исправленными опечаткмми





#==============================================================
#                ПОЛУЧЕНИЕ ДАННЫХ ИЗ PANDAS
#==============================================================
df = pd.read_csv("search_history0_lemma.csv")

#---------------ВЫДЕЛЕНИЕ НЕБОЛЬШОЙ ТЕСТОВОЙ ЧАСТИ-------------
#unique_IDs = pd.unique(df['wbuser_id'])
#groups     = df.groupby(['wbuser_id'])

#result = np.array_split(unique_IDs, 10)

#result[]

df.head()

df.columns = ['wbuser_id', 'UQ', 'cnt', 'locale', 'weekday', 'time']





df.head()



process_string ('купить ноутбук',  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer)

#Этого делать не надо так как данные будут предобработаны на локальной машине.
#df['UQ'] = df['UQ'].progress_apply((lambda x: process_string (x,  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer )))

process_string ('тапочки женские домашние',  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer)



df['wbuser_id'].value_counts()



# КОЛЛАБОРАТИВНАЯ ФИЛЬТРАЦИЯ

#test_word = 'алаfnfdnfdf'
#test_word = russian_lemmatizer.parse(test_word)[0]
#print (test_word)

df

#СОЗДАДИМ СЛОВАРЬ: СЛОВО(ЗАПРОС) НА РУССКОМ ЯЗЫКЕ - СПИСОК ПОЛЬЗОВАТЕЛЕЙ (ДАЛЕЕ БУДЕТ УРЕЗАН)
#Если точных совпадений не будет, сократим запрос до основы
word_users_dict = dict()

for index, row in df.iterrows():
  product = row['UQ']
  user    = row['wbuser_id']

  if product in word_users_dict:
    word_users_dict[product].add (user) 
  else:
    word_users_dict[product] = set ([user])

df

#wbuser_id	UQ	cnt	locale	weekday	time	wbuser_id\t

def get_dict_of_dicts(df):
  mentions = dict()
  rate    = 1

  for index, row in df.iterrows():
    user    = row['wbuser_id']
    product = row['UQ']    

    if not user in mentions:
        mentions[user] = dict()
    mentions[user][product] = rate
  
    if (user == '6be909181c81d96998b538269e324e12'):
      print (product)

  return mentions

def distCosine (vecA, vecB):
    #У нас это были спискт из слов.
    set_A = set (vecA)
    set_B = set (vecB)
    
    d = len (set_A  & set_B)

    return d / len (set_A) / len (set_B)



#МЕТРИКА ЯВЛЯЕТСЯ ПОДБИРАЕМОЙ!!!

import math

def makeRecommendation (query, userID, userRates, words_dict, nBestUsers, nBestProducts):
  #matches = [(u, distCosine(userRates[userID], userRates[u])) for u in userRates if u != userID]
  
  # Для решения проблемы с особыми пользователями 
  # Найден пользователь с очень большим число запросов
  # Вероятно, это пользователи без личного кабинета в вайлдбериз
  excluded_set = set (['70311ec9008a31f743c164e6f1198c86'])


  #Ищем множество пользователей которые похожи по данному запросу
  result_set = set()
  #Если запрос очень популярный, то используем словарь для этого запроса
  #иначе для составных слов запроса (русских слов). 
  if (query in words_dict or words_dict [query] > BIG_QUESTION):
    result_set  = words_dict [query]
    #print ('case 1')
  else:
    #print ('case 2')
    for word in query.split():
      if (re.match ('[А-Яа-я]+',word)):
        curr_set = words_dict[word]
        result_set |= curr_set    
  #print (len (result_set))
  pass
  
  #Считаем количество слов в запросах пользователя относящихся к этой тематике!!
  #
  def count_queries_another_user (this_query, another_user_queries):
    answer = 0
    list_of_words = this_query.split()
    
    for word in list_of_words:
      #(not (re.match ('.*ый', word) or re.match ('.*ий', word) or  re.match ('.*ой', word)) ) and 
      if len (word) > 3 :
        #print ('.', end=' ')
        for another_user_query in another_user_queries:
          another_user_query_split = another_user_query.split()
          
          if word in another_user_query_split:
            answer += 1
            #print ('#' + word)
      
    return answer


  # Дальше по похожим пользователям по данному запросу ищем  
  # применяя модифицированную косинусную метрику, учитывающую количество запросов совершенных пользователем относящихся к этой теме
  matches = [(u, distCosine(userRates[userID], userRates[u]  )  +   count_queries_another_user (query, userRates[u].keys()) /  NORMING_CONST  ) 
            for u in result_set if u != userID and u not in excluded_set]
  
  print (matches)

  bestMatches = sorted(matches, key=lambda x: x[1], reverse=True)
  bestMatches = bestMatches[:nBestUsers]
  #print ("Most correlated with '%s' users:", userID)
  #for line in bestMatches:
      #print ("  UserID: %6s  Coeff: %6.4f", (line[0], line[1]))    

  sim = dict()
  sim_all = sum([x[1] for x in bestMatches])
  bestMatches = dict([x for x in bestMatches if x[1] > 0.0])        
  for relatedUser in bestMatches:
      for product in userRates[relatedUser]:
          #print (product)
          #print (product, userRates[userID])
          #Выбираем похожие но не совпадающие с уже бывшими запросами у пользователя!
          if not product in userRates[userID]:
              #print (product)

              if not product in sim:
                  sim[product] = 0.0
              sim[product] += userRates[relatedUser][product] * bestMatches[relatedUser]
  for product in sim:
      sim[product] /= sim_all
  bestProducts = sorted(sim.items(), key=lambda x: x[1], reverse=True)[:nBestProducts]
  print (bestProducts)

  print ("Most correlated products:")
  #for prodInfo in bestProducts:    
      #print ("  ProductID: %6s  CorrelationCoeff: %6.4f", (prodInfo[0], prodInfo[1]))
  return [(x[0], x[1]) for x in bestProducts]

#Записываем словарь пользователей - запросов пользователей
userRates = get_dict_of_dicts(df)

userRates ['6be909181c81d96998b538269e324e12']



#Смотрим что у нас корректно все с запросами
list(word_users_dict.keys())[:10]

#ТЕСТИРОВАНИЕ
(list (word_users_dict['домашний женский тапочки']))[:10]

df['wbuser_id'].value_counts()

df['wbuser_id'].value_counts().describe()

df['UQ'].value_counts()

process_string ("женский куртка осенний", spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer)

list (word_users_dict["женский куртка осенний"]) [:10]

userRates['6be909181c81d96998b538269e324e12']

df [df['wbuser_id']=='6be909181c81d96998b538269e324e12']

for index, row in df.iterrows():
  user    = row['wbuser_id']
  product = row['UQ']    


  if (user == '6be909181c81d96998b538269e324e12'):
    print (product)







userRates ['933a3b98fde8370e594e4e79fe4ea6b4']

QUERY = 'осенняя женская куртка'

rec = makeRecommendation (process_string (QUERY,  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer), '6be909181c81d96998b538269e324e12',  userRates, word_users_dict, 500, 500)

rec[:10]

rec_without_numbers = [record [0] for  record  in rec[:10]]
rec_without_numbers[:10]



rec_without_numbers_query = []
QUERY_split = (process_string (QUERY,  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer)).split()
print (QUERY_split)

for   record  in rec_without_numbers[:10]:
  tmp_split = record.split()
  print (tmp_split)

  tmp_without_query = ""
  for word_tmp in tmp_split:
    if (word_tmp not in QUERY_split):
      tmp_without_query += word_tmp + ' ' 
  
  tmp_without_query = tmp_without_query.strip()

  rec_without_numbers_query.append ( tmp_without_query )

rec_without_numbers_query

main_set = set()

for elem in rec_without_numbers_query:
  for word in elem.split():
    if (len(main_set) > 10):
      break;

    #main_set.add (word)
    
    word = russian_lemmatizer.parse(word)[0]
    if not ('NOUN' in word.tag):
      main_set.add (word.word)







from google.colab import drive
drive.mount('/content/drive')

QUERY_split = (process_string (QUERY,  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer)).split()

query_nouns = []
#morph.parse('стали')[0]
for word in QUERY_split:
  word = russian_lemmatizer.parse(word)[0]
  if ('NOUN' in word.tag):
    query_nouns.append (word.word)

query_nouns



import gensim

#Подгрузка 
model_path = '/content/drive/MyDrive/wildhack/noun_tayga_1_2.vec'
model_ru = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)

query_nouns

#words = process_string (QUERY,  spell_russian,  spell_english,     russian_lemmatizer,  english_lemmatizer).split()
common_words = []

for word in query_nouns:
    # есть ли слово в модели? 
    if word in model_ru:
        for i in model_ru.most_similar(positive=[word], topn=10):
            # слово + коэффициент косинусной близости

            if (i[1] > PROXIMITY):
              common_words.append( i[0] )
    else:
        print('Cлова "%s" в модели нет!' % word)

print(common_words)

additional_tags = common_words





#ИТОГОВЫЙ РЕЗУЛЬТАТ, не успели его отсортировать

print ('Выбранные теги', query_nouns)
print ('Дополнительные теги', common_words)
print ('Основные теги', main_set)